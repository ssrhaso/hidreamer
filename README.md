Trained lightweight CNN Encoder + discrete tokens + attention dynamics = sample-efficient Atari control on a single GPU. Research engineering sprint combining three validated techniques in a novel architecture

## Results (AT PRESENT)

**CNN Encoder (1.8M params, 30 epochs):**

- Temporal consistency: 0.260 (excellent - consecutive frames 74% closer than random)
- Embedding diversity: 1.411 (no collapse)
- Game state correlation: 0.267 (3x better than DINOv2's 0.08, sufficient for dynamics)

**VQ Tokenizer (256 codes, 50 epochs):**

- Codebook usage: 256/256 (100% - perfect utilization)
- Perplexity: 247.52 (near theoretical max of 256)
- Training time: 50 epochs in 50 seconds

**Multi-Game Generalization (Pong, Breakout, MsPacman):**

* **Unified Embedding Space:** A single 1.8M param encoder learned consistent temporal features across 3 distinct games (Norm: 1.0, Temporal Ratio: 0.22 avg).
* **Shared Vocabulary:** Trained one VQ codebook (256 codes) on 300k combined frames.
* **Zero-Shot Transfer?** No, but "few-shot adaptation" proven: 100% codebook usage and high perplexity (~240) achieved on the joint dataset in just 2 epochs. This validates that our discrete latent space is robust enough to represent diverse game physics simultaneously.
* 

**Key Insight:** Strong temporal structure (0.260) matters more than semantic correlation for discrete world models. The encoder's ability to group consecutive frames enabled perfect VQ codebook utilization, validating the contrastive learning approach.

## Lessons Learned

- We initially projected DINOv2 embeddings from 384 to 128 dimensions using an MLP, but found this introduced unnecessary information loss. We removed the projection and fed 384-dimensional embeddings directly to the VQ layer, improving codebook utilization from 77/256 to 153/256.
- Frozen DINOv2 embeddings failed on Atari with only 0.08 correlation to game states (paddle position: 0.05, ball position: 0.02-0.15). While temporal consistency (0.53) showed DINOv2 detects motion, it couldn't capture positionâ€”the foundation model's natural image features don't transfer to synthetic game graphics. This 10x performance gap vs trainable CNNs demonstrates when vision foundation models fail: domain shift matters more than model scale. We pivoted to a trained CNN, proving task-specific encoders outperform general-purpose foundations on domain-shifted tasks.
-
