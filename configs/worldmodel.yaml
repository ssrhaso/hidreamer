# BASELINE IMPLEMENTATION OF WORLD MODEL

model:
  # ARCHITECTURE - STORM INSPIRED [Zhang et al. 2023] 

    d_model :  384          # EMBEDDING DIMENSION (WIDTH OF NN)             - HRVQ EMBEDDING DIMENSION
    n_layers :  6           # NUMBER OF TRANSFORMER BLOCKS (DEPTH OF NN)    - (Kaplan et al. 2020)
    n_heads :  6            # NUMBER OF ATTENTION HEADS                     - (Vaswani et al. 2017) - standard rule of = d_model / 64
    d_ff :  1536            # DIMENSION OF FEEDFORWARD NETWORK              - (Vaswani et al. 2017) - standard rule of = 4 * d_model
    dropout :  0.1          # DROPOUT RATE                                  - (Devlin et al. 2017)  - BERT, GPT, STORM use 0.1       
    max_seq_len :  256      # MAXIMUM SEQUENCE LENGTH                       - ~ 65k positions of memory, Fits T4 GPU safely
    num_codes :  256        # NUMBER OF CODEBOOK ENTRIES                    - HRVQ Codebook Size
    num_actions :  9        # NUMBER OF POSSIBLE ACTIONS                    - ATARI100K has 9 discrete actions
    
    # HIERARCHICAL LOSS (NOVELTY)
    layer_weights : None


# TRAINING
training: 
  batch_size: 32
  seq_len: 64
  learning_rate: 0.0003
  weight_decay: 0.1
  grad_clip: 1.0
  num_epochs: 50


# DATA
data:
  games: ['ALE_Pong-v5', 'ALE_Breakout-v5', 'ALE_MsPacman-v5']
  tokens_dir: 'checkpoints/rsvq_tokens'
  replay_dir: 'data'
  val_split: 0.05

# LOGGING
logging:
  save_dir: 'checkpoints/world_model'
  save_every: 10