Trained lightweight CNN Encoder + discrete tokens + attention dynamics = sample-efficient Atari control on a single GPU. Research engineering sprint combining three validated techniques in a novel architecture

- We initially projected DINOv2 embeddings from 384 to 128 dimensions using an MLP, but found this introduced unnecessary information loss. We removed the projection and fed 384-dimensional embeddings directly to the VQ layer, improving codebook utilization from 77/256 to 153/256.

- Frozen DINOv2 embeddings failed on Atari with only 0.08 correlation to game states (paddle position: 0.05, ball position: 0.02-0.15). While temporal consistency (0.53) showed DINOv2 detects motion, it couldn't capture positionâ€”the foundation model's natural image features don't transfer to synthetic game graphics. This 10x performance gap vs trainable CNNs (0.8+ correlation) demonstrates when vision foundation models fail: domain shift matters more than model scale. We pivoted to a trained CNN, proving task-specific encoders outperform general-purpose foundations on domain-shifted tasks.