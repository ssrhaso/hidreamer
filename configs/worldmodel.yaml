# BASELINE IMPLEMENTATION OF WORLD MODEL

model:
  # ARCHITECTURE - STORM INSPIRED [Zhang et al. 2023] 

    d_model :  384          # EMBEDDING DIMENSION (WIDTH OF NN)             - HRVQ EMBEDDING DIMENSION
    n_layers :  6           # NUMBER OF TRANSFORMER BLOCKS (DEPTH OF NN)    - (Kaplan et al. 2020)
    n_heads :  6            # NUMBER OF ATTENTION HEADS                     - (Vaswani et al. 2017) - standard rule of = d_model / 64
    d_ff :  1536            # DIMENSION OF FEEDFORWARD NETWORK              - (Vaswani et al. 2017) - standard rule of = 4 * d_model
    dropout :  0.1          # DROPOUT RATE                                  - (Devlin et al. 2017)  - BERT, GPT, STORM use 0.1       
    max_seq_len :  256      # MAXIMUM SEQUENCE LENGTH                       - ~ 65k positions of memory, Fits T4 GPU safely
    num_codes :  256        # NUMBER OF CODEBOOK ENTRIES                    - HRVQ Codebook Size
    num_actions :  9        # NUMBER OF POSSIBLE ACTIONS                    - ATARI100K has 9 discrete actions
    
    # HIERARCHICAL LOSS (NOVELTY)
    layer_weights : [1.0, 0.5, 0.1]   # WEIGHTS FOR EACH HRVQ LAYER LOSS    # HIGHER WEIGHTING FOR FINAL PIXEL RECONSTRUCTION, 
                                                                            # LOWER FOR COARSE REPRESENTATION






# TRAINING
training: 
  batch_size: 32
  seq_len: 64
  learning_rate: 0.0003
  weight_decay: 0.1
  grad_clip: 1.0
  num_epochs: 50

  warmup_steps: 1000      # Linear warmup before cosine decay
  betas : [0.9, 0.95 ]    # AdamW momentum parameters (GPT/STORM)
  mixed_precision : true  # float16 AMP on T4 (adjustable)
  accumulation_steps : 2  # Effective batch, 32x2 = 64, to fit T4 GPU memory constraints (adjustable)


# DATA
data:
  games: ['Pong-v5', 'Breakout-v5', 'MsPacman-v5']
  tokens_dir: 'checkpoints/rsvq_tokens'
  replay_dir: 'data'
  val_split: 0.05

# LOGGING
logging:
  save_dir: 'checkpoints/world_model'
  save_every: 10
